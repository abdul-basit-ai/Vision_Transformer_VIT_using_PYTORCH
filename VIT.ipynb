{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "636d3a75",
      "metadata": {
        "id": "636d3a75"
      },
      "source": [
        "Hello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70165ada",
      "metadata": {
        "id": "70165ada"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d8ea1f",
      "metadata": {
        "id": "13d8ea1f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054c2f2b",
      "metadata": {
        "id": "054c2f2b"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6429087",
      "metadata": {
        "id": "d6429087"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "EMBED_DIM = 256\n",
        "CHANNELS = 3\n",
        "NUM_HEADS = 8\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROPOUT = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ce2838",
      "metadata": {
        "id": "d5ce2838"
      },
      "outputs": [],
      "source": [
        "#IMAGE TRANSFORMS\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c3c048",
      "metadata": {
        "id": "38c3c048"
      },
      "outputs": [],
      "source": [
        "#DataSet\n",
        "train_dataset= datasets.CIFAR10(root='data', train=True, download=True, transform=transforms)\n",
        "test_dataset= datasets.CIFAR10(root='data', train=False, download=True, transform=transforms)\n",
        "#DataLoader\n",
        "train_loader= DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader= DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(f'Number of training samples: {len(train_dataset)}')\n",
        "print(f'Number of test samples: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7a323a",
      "metadata": {
        "id": "4a7a323a"
      },
      "outputs": [],
      "source": [
        "#Vision Transformer (ViT) Model from Scratch\n",
        "class Patch_Embedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels,embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        num_patcehes = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patcehes + 1, embed_dim))\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]  # Get the batch size\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "    # Corrected line: expand the cls_token tensor\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        return (x)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "376d799b",
      "metadata": {
        "id": "376d799b"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, dropout):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x= self.dropout(F.gelu(self.fc1(x)))\n",
        "        x= self.dropout(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d445a33",
      "metadata": {
        "id": "8d445a33"
      },
      "outputs": [],
      "source": [
        "class Transformer_Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm1 = self.norm1(x)\n",
        "        attn_output, _ = self.attn(x_norm1, x_norm1, x_norm1)\n",
        "        x = x + attn_output  # Residual connection\n",
        "        x_norm2 = self.norm2(x)\n",
        "        x = x + self.mlp(x_norm2)  # Residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff44e64",
      "metadata": {
        "id": "cff44e64"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.patch_embed = Patch_Embedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            Transformer_Encoder(embed_dim, num_heads, mlp_dim, dropout) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)  # [B, N+1, D]\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token_final = x[:, 0]  # CLS token\n",
        "        x = self.head(cls_token_final)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kaJcA3vqpuB"
      },
      "outputs": [],
      "source": [
        "#Instant\n",
        "model = ViT(IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES, EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROPOUT).to(device)\n",
        "model"
      ],
      "id": "2kaJcA3vqpuB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c667c5b",
      "metadata": {
        "id": "1c667c5b"
      },
      "outputs": [],
      "source": [
        "#loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879711e8",
      "metadata": {
        "id": "879711e8"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def Train_Vit(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f050c7e4",
      "metadata": {
        "id": "f050c7e4"
      },
      "outputs": [],
      "source": [
        "def eval1(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68a8cc7",
      "metadata": {
        "id": "c68a8cc7"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "train_accuracies, test_accuracies = [], []\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = Train_Vit(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = eval1(model, test_loader)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FYV8Xs1_xxuZ"
      },
      "id": "FYV8Xs1_xxuZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def predict_and_plot_grid(model,\n",
        "                          dataset,\n",
        "                          classes,\n",
        "                          grid_size=3):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            img, true_label = dataset[idx]\n",
        "            input_tensor = img.unsqueeze(dim=0).to(device)\n",
        "            with torch.inference_mode():\n",
        "                output = model(input_tensor)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "            img = img / 2 + 0.5\n",
        "            npimg = img.cpu().numpy()\n",
        "            axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "            color = classes[true_label] == classes[predicted.item()]\n",
        "            if color:\n",
        "                c = \"g\"\n",
        "            else:\n",
        "                c = \"r\"\n",
        "            axes[i, j].set_title(f\"Truth: {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\", fontsize=10, c=c)\n",
        "            axes[i, j].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-m7kEo80xymw"
      },
      "id": "-m7kEo80xymw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_plot_grid(model, test_dataset, classes=train_dataset.classes, grid_size=3)"
      ],
      "metadata": {
        "id": "HkFkq5TNxyqS"
      },
      "id": "HkFkq5TNxyqS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WC9TFHs-xyuL"
      },
      "id": "WC9TFHs-xyuL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}